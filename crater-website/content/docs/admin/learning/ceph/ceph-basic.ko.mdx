---
title: "Rook-Ceph 연습 가이드"
description: "Rook-Ceph 초보자용 가이드입니다!"
---

> 이 가이드는 Ceph 와 분산 스토리지 경험 전혀 없는 분들을 대상으로 하며, Rook-Ceph 를 빠르게 이해하고 실제로 사용하는 방법을 안내합니다.

## 1. 전제 배경

Rook-Ceph 를 실습하기 전에, **Ceph**와 **Rook-Ceph**라는 두 가지 핵심 개념을 근본적으로 이해하는 것이 중요합니다. 이들은 분산 스토리지 시스템의 핵심 역량과 Kubernetes 상의 자동화 운영 솔루션을 각각 나타냅니다.

### 1. Ceph

**Ceph 는 오픈소스 분산 스토리지 시스템**으로, Sage Weil 이 시작했으며, 대규모 스토리지 환경에서 데이터 일관성, 가용성 및 확장성을 해결하기 위해 설계되었습니다. 전통적인 집중형 스토리지 솔루션과 달리 Ceph 는 단일 컨트롤러에 의존하지 않고, 대등한 노드 간 협업을 기반으로 합니다. Ceph 의 핵심 설계 목표는 다음과 같습니다:

* **고확장성**: 수 TB 에서 수 PB 까지의 저장 용량 증가를 지원합니다.
* **고가용성 및 내결함성**: 복제본 또는 오류 교정 코드 메커니즘을 통해 노드 다운타임 시에도 데이터가 사용 가능하도록 보장합니다.
* **통합 스토리지 모델**: 블록 장치 (Block), 오브젝트 스토리지 (Object), 파일 시스템 (File) 의 접근 방식을 모두 지원합니다.

Ceph 클러스터는 주로 다음 핵심 구성 요소로 구성됩니다:

| 구성 요소 | 기능 설명 |
| --- | --- |
| MON(Monitor) | 클러스터 상태, 건강 점검, 주 장치 선출 등 메타데이터 서비스를 제공합니다. |
| OSD(Object Storage Daemon) | 데이터의 실제 읽기/쓰기, 복제, 복구 등 핵심 기능을 담당합니다. |
| MGR(Manager) | 모니터링, 통계 및 플러그인 확장 기능을 제공합니다. |
| MDS(Metadata Server, 선택 사항) | 파일 시스템 지원 시 디렉터리 트리 메타데이터를 관리합니다. |

일반적으로 Ceph 클러스터는 3 개 이상의 MON 이 필요하며, 여러 OSD 가 각 노드의 물리 디스크에 마운트되어 데이터 저장 능력을 공유합니다.

### 2. Rook-Ceph

**Rook 은 Kubernetes 기반의 스토리지 오케스트레이터 (Operator)**이며, **Pod 처럼 스토리지 시스템을 관리할 수 있도록 설계되었습니다**. Rook-Ceph 는 이 중 가장 성숙하고 널리 사용되는 백엔드 중 하나이며, Ceph 의 Kubernetes 상의 배포 및 관리 과정을 간소화하는 데 사용됩니다.

Rook-Ceph 는 Ceph 의 배포 및 유지 관리를 Kubernetes 의 사용자 정의 리소스 (Custom Resources) 로 추상화합니다. 이는 다음과 같습니다:

| 사용자 정의 리소스 | 설명 |
| --- | --- |
| `CephCluster` | 완전한 Ceph 클러스터 선언을 나타냅니다. |
| `CephBlockPool` | Ceph 의 RBD 블록 스토리지 풀을 관리합니다. |
| `CephFilesystem` | Ceph 파일 시스템을 관리합니다. |
| `CephObjectStore` | 오브젝트 스토리지 버킷 서비스를 관리합니다. |

Operator 모델을 통해 Rook 은 Ceph 클러스터의 배포, 업그레이드, 장애 복구, 확장 및 축소 등 생명주기 관리를 실현하며, 전통적인 Ceph 설치 과정의 복잡성을 크게 줄입니다.

### 3. Kubernetes 에서 Ceph 사용

Kubernetes 는 본래 의미상의 지속 가능한 저장을 제공하지 않습니다. 만약 Pod 이 PVC(PersistentVolumeClaim) 에 바인딩된 경우, 신뢰할 수 있는 백엔드가 없으면 고가용성, 크로스 노드, 자동 복구 기능을 갖춘 저장 기능을 달성할 수 없습니다. Ceph, 특히 Rook 과 통합된 Ceph 는 이 부족한 부분을 보완합니다.

* 신뢰성 있는 데이터 지속 저장;
* 탄력적 확장;
* 고가용성 블록 장치와 공유 파일 시스템 지원;
* Kubernetes 에서 CSI 플러그인과 StorageClass 동적 제공을 지원합니다.

Rook-Ceph 는 진정한 클라우드 네이티브 스토리지 솔루션을 구축할 수 있습니다.

## 2. 학습 경로

Ceph 와 Rook-Ceph 는 내용이 복잡하며, 초보자에게서 가장 큰 어려움은 자료가 부족하지 않지만, 체계성과 단계성이 부족하다는 점입니다. 효율을 높이고 비효율적인 실수를 피하기 위해, 학습 경로를 세 단계로 나누고, 실무에서 관련 도구에 대한 인식을 구축하는 것이 좋습니다.

### 1. 단계 1: 개념 확립

#### 추천 자료

- 공식 문서: https://rook.github.io/docs/rook/latest-release/Getting-Started/intro/
- 중국어 인트로: Bilibili 에서 Ceph 입문 시리즈 검색
- ChatGPT 등으로 용어 확인

#### 핵심 목표: Ceph 와 Rook-Ceph 의 설계 동기와 관계를 이해하고, 핵심 구성 요소의 역할을 분명히 하기

| 이해해야 할 문제 | 추천 질문 형식 | 왜 중요한가 |
| --- | --- | --- |
| Ceph 는 어떤 문제를 해결하기 위해 탄생했나요? | Ceph 와 전통적인 집중형 스토리지 시스템의 근본적인 차이를 간략히 설명하십시오. | Ceph 의 분산 + 내결함성 + 분산화라는 핵심 아이디어를 명확히 하기 위해. |
| Ceph 와 Rook-Ceph 는 어떤 관계인가요? | K8s 에서 Rook-Ceph 를 사용하면 Ceph 를 사용하는 것이며, Rook 은 어떤 역할을 하나요? | Rook 이 Ceph 의 운영 로직을 포장한다는 개념을 이해하고, 둘을 혼동하지 않도록 하기 위해. |
| Ceph 의 핵심 구성 요소는 무엇이며, 각각 무엇을 담당하나요? | MON, OSD, MGR, MDS 의 역할과 상호작용을 설명하십시오. | 이후 모든 명령어 조작 및 장애 해결의 입구를 명확히 하기 위해. |
| RBD, 오브젝트 스토리지, 파일 시스템이 무엇이며, Kubernetes 에서 사용하는 것은 무엇인가요? | Kubernetes 에서 PVC 를 사용할 때는 Ceph 의 블록 스토리지가 아닌 다른 형태를 사용하나요? | Ceph 가 블록 스토리지인지, 클라우드 디스크 등 다른 개념을 포함하는지 명확히 하기 위해. |
| Kubernetes 에서 Ceph 를 배포할 때 왜 Rook 을 사용하나요? | Rook 없이도 직접 Ceph 를 배포할 수 있나요? 차이점은 무엇인가요? | Kubernetes 와 전통 아키텍처의 차이, 특히 선언형 리소스 아이디어를 이해하기 위해. |

### 2. 단계 2: 환경 익히기

#### 기본 목표

- K8s 에서 toolbox 에 들어가기, 기본 진단 및 상태 명령 실행 방법을 배우기;
- `ceph status`, `osd tree` 등 명령어의 의미를 읽고 이해할 수 있도록;
- 기본적으로 `kubectl`을 사용하여 Rook 리소스 객체를 관리할 수 있도록.

#### 핵심 목표: Rook-Ceph 배포 후 클러스터 내의 리소스를 이해하고, toolbox 와 K8s 객체 간의 연결 관계를 파악하며, 기본적인 쿼리 작업을 독립적으로 수행할 수 있도록.

| 이해해야 할 문제 | 추천 질문 형식 | 왜 중요한가 |
| --- | --- | --- |
| 어떻게 toolbox 에 들어가고, 이는 실제로 어디에 연결되어 있나요? | `rook-ceph-tools`라는 Pod 은 Ceph 클러스터에 어떻게 연결되어 있나요? Ceph 도 실행되고 있나요? | toolbox 가 Ceph 자체가 아니라 원격 클라이언트라는 오해를 피하기 위해. |
| `ceph status`에서 나온 `HEALTH_WARN`은 무엇을 의미하며, 어떻게 추적해야 하나요? | `ceph health detail`에서 `mon quorum lost`는 어떤 상황인가요? | 각 진단 출력에는 문제의 단서가 포함되어 있으며, 이를 단계별로 추적해야 합니다. |
| Ceph 는 어떻게 디스크를 스토리지 자원으로 만드나요? OSD 와 디스크는 어떤 관계인가요? | OSD 는 물리 디스크와 어떻게 대응되나요? 여러 OSD 가 동일한 디스크를 공유할 수 있나요? | 이후 `ceph osd down` 또는 디스크 고장 시의 작업 논리를 배우기 위해. |
| Ceph 의 Pool 은 무엇이며, 각 PVC 는 왜 Pool 에 해당하나요? | 왜 여러 개의 Pool 이 필요한가요? Pool 의 복제 수와 성능, 공간 사이의 관계는 무엇인가요? | Pool 은 분할 자원 단위이며, 이를 모르면 성능 최적화 및 재해 대응 전략을 설정할 수 없습니다. |
| Kubernetes 의 StorageClass 와 Ceph 의 BlockPool 은 어떻게 연결되나요? | PVC 에서 Ceph RBD 로의 자원 매핑은 어떻게 이루어지나요? RBD 는 누가 생성하나요? | K8s → Rook → Ceph 의 제어 체인을 이해하고, 데이터 저장 위치와 부하 속성을 분석하기 위해. |

### 3. 단계 3: 직접 실습

#### 기본 목표

- Ceph 클러스터 일상적인 유지 관리 기초 작업을 익히기;
- 저장 풀 생성 후 PVC 에 바인딩하여 애플리케이션을 실행하는 흐름을 완료할 수 있도록;
- 대부분의 일반적인 오류를 독립적으로 진단하고 원인을 파악할 수 있도록.

#### 핵심 목표: Pool, RBD, PVC 를 생성 및 조작하고, 일반적인 오류를 처리하며, Rook 업그레이드 및 Ceph 작업 체인을 이해할 수 있도록.

| 이해해야 할 문제 | 추천 질문 형식 | 왜 중요한가 |
| --- | --- | --- |
| 특정 RBD 가 올바르게 생성되었고, 특정 PVC 에 바인딩되었는지 어떻게 판단하나요? | PVC 로부터 해당하는 Ceph RBD 이름을 역추적하는 방법은 무엇인가요? | 저장이 올바르게 구성되었는지 확인하는 방법, 특히 마운트 실패 시의 진단을 배우기 위해. |
| Pool 의 복제 수 설정이 성능 및 내결함성에 어떤 영향을 주나요? | 3 복제 풀에서는 반드시 3 배의 공간이 소비되는가요? 만약 두 대의 머신만 사용한다면 어떻게 되나요? | Pool 매개변수가 성능과 용량에 미치는 영향은 최적화의 핵심 기술입니다. |
| 스냅샷을 생성한 후 어떻게 복구하고, 이는 기존 데이터에 영향을 주나요? | Ceph RBD 스냅샷 rollback 은 기존 데이터를 덮어쓰나요? 복구 작업을 어떻게 시연하나요? | 스냅샷은 실험/테스트 시 중요한 도구이며, 실수로 인한 데이터 삭제를 방지하는 데 사용됩니다. |
| 왜 `rook-ceph-osd-xxx` pod 가 죽었나요? 어떻게 분석하나요? | OSD pod 가 시작 실패 시, 로그나 ceph 명령어로 원인을 분석하는 방법은 무엇인가요? | 이러한 문제는 자주 발생하므로, 로그 진단과 하드웨어 매핑 논리를 알아야 합니다. |
| Rook 업그레이드 과정에서 어떤 핵심 지표를 관찰해야 하나요? | v1.14 에서 v1.15 로 업그레이드 시, 어떤 pod 가 먼저 업데이트되어야 하고, 어떤 자원은 변경할 수 없는가요? | 무중단 업그레이드를 학습하고, 저장 데이터 손실을 방지해야 합니다. |

> **팁**
>
> - 실제 생성 → 마운트 → 쓰기 → 삭제의 완전한 흐름을 수행하십시오;
> - 실수를 두려워하지 말고, 특히 테스트 환경에서 의도적으로 OSD down/MON 장애 등을 유도하십시오;
> - 오류 메시지와 CRD 필드 용도에 대한 대규모 모델의 설명을 자주 요청하십시오.

## 3. 자주 사용하는 명령어

### 1. 기본 정보 확인 및 건강 상태 점검

| 작업 | 명령 | 용도 설명 |
| --- | --- | --- |
| 전체 건강 상태 확인 | `ceph -s` 또는 `ceph status` | 클러스터가 `HEALTH_OK` 상태인지 확인하고, 아니라면 경고 내용을 분석하십시오. |
| 클러스터 공간 사용량 확인 | `ceph df` | 어떤 pool 이 가장 많은 공간을 사용하고 있는지 확인하십시오. |
| 모든 구성 요소 버전 확인 | `ceph versions` | 업그레이드 후 버전이 통일되었는지 확인하십시오. |
| 클러스터 호스트 노드 토폴로지 확인 | `ceph osd tree` | OSD 분포를 확인하고, 장애 노드를 식별하는 데 특히 중요합니다. |
| MON 정보 확인 | `ceph mon dump` | quorum 수와 장애 복구 요구 사항을 확인하십시오. |
| 클러스터 경고 세부 정보 확인 | `ceph health detail` | 각 경고의 구체적인 정보와 구성 요소 위치를 확인하십시오. |

### 2. OSD 작업

| 작업 | 명령 | 용도 설명 |
| --- | --- | --- |
| 모든 OSD 목록 확인 | `ceph osd ls` | OSD ID 를 확인하십시오. |
| OSD 상태 확인 | `ceph osd stat` | 모든 OSD 가 up/in 상태인지 확인하십시오. |
| 특정 OSD 의 상세 상태 확인 | `ceph osd dump` | OSD in/out 상태를 파악하는 데 자주 사용됩니다. |
| 특정 OSD 를 하선 처리 | `ceph osd out <id>` | 디스크/유지 보수 노드를 계획적으로 하선할 때 사용됩니다. |
| 특정 OSD 를 다시 올라오게 하기 | `ceph osd in <id>` | 디스크 사용 또는 오류 복구를 위해 사용됩니다. |
| 특정 OSD 를 수동으로 장애 상태로 표시 | `ceph osd down <id>` | 장애 복구 시나리오의 테스트 작업에 사용됩니다. |
| OSD 재균형 트리거 | `ceph osd reweight-by-utilization` | 데이터 분포가 불균형할 때 사용됩니다. |

### 3. Pool 관리

| 작업 | 명령 | 용도 설명 |
| --- | --- | --- |
| 기존 Pool 확인 | `ceph osd pool ls` | 현재의 저장 구조를 빠르게 파악하십시오. |
| Pool 세부 정보 확인 | `ceph osd pool get <pool> all` | 복제 수, 인코딩 방식 등 매개변수를 확인하십시오. |
| Pool 생성 | `ceph osd pool create mypool 128 128` | 실험 환경에서 다양한 크기의 Pool 을 시도할 수 있습니다. |
| Pool 삭제 | `ceph osd pool delete mypool mypool --yes-i-really-really-mean-it` | 주의 깊게 사용하십시오. 실험 환경에서만 사용하십시오. |
| 복제 수를 3 으로 설정 | `ceph osd pool set mypool size 3` | 데이터의 중복도를 보장하지만, 공간 사용량이 증가합니다. |

### 4. RBD 작업 연습

| 작업 | 명령 | 용도 설명 |
| --- | --- | --- |
| RBD 이미지 생성 | `rbd create myrbd --size 1024 --pool mypool` | PVC 에 바인딩하기 전에 이미지를 생성해야 합니다. |
| 이미지 확인 | `rbd ls -p mypool` | 생성 여부를 확인하십시오. |
| 이미지 정보 확인 | `rbd info mypool/myrbd` | 이미지의 크기와 사용 상황을 확인하십시오. |
| 이미지 삭제 | `rbd rm mypool/myrbd` | 테스트용 이미지를 삭제하고 공간을 해제하십시오. |
| 스냅샷 생성 | `rbd snap create mypool/myrbd@snap1` | 스냅샷과 복구 기능 테스트에 사용됩니다. |
| 스냅샷 롤백 | `rbd snap rollback mypool/myrbd@snap1` | 실수로 인한 데이터 복구를 시뮬레이션합니다. |

### 5. 파일 시스템 작업 (MDS 가 배포된 경우에만 가능)

| 작업 | 명령 | 용도 설명 |
| --- | --- | --- |
| 파일 시스템 생성 | `ceph fs new myfs myfs_metadata myfs_data` | NFS 와 유사한 공유 마운트를 지원합니다. |
| 모든 파일 시스템 확인 | `ceph fs ls` | 파일 시스템이 정상적으로 작동하는지 확인하십시오. |
| 파일 시스템 상태 확인 | `ceph fs status` | 활성 MDS 노드와 클라이언트 연결 수를 확인하십시오. |

## 4. 연습 시나리오

### 1. 시나리오 1: 대규모 모델 훈련 서비스를 위한 독립 데이터 볼륨 생성

실험실에서 PyTorch 기반의 대규모 모델 훈련 서비스를 배포했으며, 각 사용자가 독립적인 데이터 저장 볼륨을 훈련 컨테이너에 마운트하려 합니다. 당신은 관리자로서 다음을 수행해야 합니다:

1. `ml-training-pool`이라는 이름의 블록 스토리지 풀을 생성하여 모델 훈련 데이터에 전용으로 사용합니다;
2. 이 풀을 Kubernetes 의 StorageClass 에 바인딩하여 사용자가 PVC 에서 사용할 수 있도록 합니다;
3. PVC 를 생성하고, 이가 성공적으로 바인딩되어 Ceph 클러스터 내에서 대응하는 RBD 이미지가 생성되었는지 확인합니다;
4. toolbox 를 사용하여 이 블록 장치의 상태, 용량 등의 세부 정보를 확인하고, 가용성을 확인합니다.

### 2. 시나리오 2: 노드 디스크 장애 시뮬레이션 및 Ceph 데이터 재균형 과정

Ceph 의 생산 환경에서의 내결함성 능력을 연습하기 위해, 하나의 OSD 노드의 비정상 하선을 시뮬레이션하고 Ceph 의 자동 복구 메커니즘이 작동하는지 관찰해야 합니다.

1. 기존 클러스터에서 하나의 OSD 를 선택하고, 이를 `out` 상태로 표시하여 디스크 오프라인을 시뮬레이션합니다;
2. Ceph 가 데이터 이동, 재균형 및 건강 상태 변화를 트리거하는지 관찰합니다;
3. OSD tree 와 클러스터 상태 변화를 확인하고, 복제본 재구성 및 용량 재분배 과정을 기록합니다;
4. 장애가 복구된 후, 이 OSD 를 `in` 상태로 표시하고 시스템 복구 과정을 관찰합니다.

### 3. 시나리오 3: 스냅샷 및 롤백을 활용한 훈련 데이터 보호 및 복구

대규모 모델 훈련 전에 사용자의 훈련 데이터 볼륨에 복구 가능한 스냅샷을 제공하여 실수로 인한 데이터 삭제를 방지하려 합니다. 시뮬레이션 시나리오는 다음과 같습니다:

1. 기존 블록 스토리지 볼륨 (대응 PVC) 에 `@pretrain`이라는 이름의 스냅샷을 생성합니다;
2. 사용자가 중요한 훈련 데이터를 실수로 삭제하거나, 중간에 이상한 데이터를 입력했을 경우를 가정합니다;
3. Ceph 의 스냅샷 롤백 기능을 사용하여 RBD 볼륨을 훈련 전의 스냅샷 상태로 복구합니다;
4. 롤백 후 데이터 일관성을 검증하고, 스냅샷이 시스템 성능과 용량에 미치는 영향을 이해합니다.

### 4. 시나리오 4: 분산 훈련 로그 수집을 위한 공유 볼륨 배포

실험실에서 분산 훈련 실험을 계획하고 있으며, 여러 Pod 인스턴스가 동일한 디렉터리에 훈련 로그를 쓰기를 수행해야 합니다. 당신은 다음을 수행해야 합니다:

1. Ceph 파일 시스템 (CephFS) 을 생성하고, MDS 서비스를 통해 POSIX 인터페이스를 제공합니다;
2. 여러 Pod 가 동시에 마운트할 수 있는 ReadWriteMany 를 지원하는 StorageClass 를 구성합니다;
3. PVC 를 생성하고, 여러 Pod 에 마운트하여 로그 동시 쓰기 시나리오를 시뮬레이션합니다;
4. 쓰인 데이터의 일관성을 검증하고, 쓰기 충돌 또는 권한 문제가 있는지 확인합니다.

### 5. 시나리오 5: 저장 풀 용량 고갈 대응 전략 시뮬레이션

훈련 작업이 집중적으로 제출되는 고강도 시기, 특정 Ceph 블록 스토리지 풀의 남은 용량이 부족해질 것으로 예상됩니다. 당신은 다음과 같이 대응 전략을 수행해야 합니다:

1. toolbox 를 사용하여 각 Pool 의 사용률과 복제 설정을 확인합니다;
2. 현재 Pool 내에서 사용되지 않고, 바인딩되지 않은 RBD 이미지가 있는지 분석하고, 정리/회수를 고려합니다;
3. 실제로 용량이 부족한 경우, 특정 Pool 의 복제 수를 3 에서 2 로 감소시켜 (테스트 환경에 한함) 대응합니다;
4. 확장 가능성 평가 후, 새로운 노드 또는 OSD 를 추가하고 시스템이 자동으로 재균형되는지 관찰합니다.

### 6. 시나리오 6: 사용되지 않는 이미지 및 Pool 정리하여 저장 공간 해제

클러스터 내에 사용되지 않는 PVC 및 대응하는 RBD 이미지가 여러 개 존재하며, 사용자가 수동으로 정리하지 못했습니다. 당신은 다음과 같이 정리 작업을 수행해야 합니다:

1. 이러한 PVC 가 소속된 Pod 들이 이미 삭제되었는지 확인합니다;
2. 이러한 PVC 가 바인딩된 RBD 이미지를 찾아서 현재 사용 중이지 않은지 확인합니다;
3. toolbox 를 사용하여 이러한 이미지를 삭제하고, 더 이상 사용되지 않는 BlockPool(의존성이 없는 경우) 을 정리합니다;
4. 클러스터 총 용량 변화를 다시 확인하여 공간이 성공적으로 회수되었는지 확인합니다.